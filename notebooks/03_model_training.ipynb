{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c503429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CORN YIELD PREDICTION - ML TRAINING PIPELINE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CORN YIELD PREDICTION - ML TRAINING PIPELINE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b768df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/8] Loading data...\n",
      "  âœ“ Loaded 82,436 records\n",
      "  âœ“ Features: 50\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "CV_FOLDS = 5\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[1/8] Loading data...\")\n",
    "\n",
    "df = pd.read_csv('modeling_dataset_final.csv')\n",
    "print(f\"  âœ“ Loaded {len(df):,} records\")\n",
    "print(f\"  âœ“ Features: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5548f7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State ANSI</th>\n",
       "      <th>County ANSI</th>\n",
       "      <th>Yield_BU_ACRE</th>\n",
       "      <th>Area_Planted_ACRES</th>\n",
       "      <th>Area_Harvested_ACRES</th>\n",
       "      <th>Production_BU</th>\n",
       "      <th>Abandonment_Rate</th>\n",
       "      <th>Harvest_Efficiency</th>\n",
       "      <th>Soil_AWC</th>\n",
       "      <th>...</th>\n",
       "      <th>heat_moisture_stress</th>\n",
       "      <th>rh_mean</th>\n",
       "      <th>rh_reproductive</th>\n",
       "      <th>weeks_high_humidity</th>\n",
       "      <th>temp_early_vs_late</th>\n",
       "      <th>precip_early_vs_late</th>\n",
       "      <th>gdd_anomaly</th>\n",
       "      <th>precip_anomaly_mm</th>\n",
       "      <th>precip_anomaly_pct</th>\n",
       "      <th>temp_anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>8.243600e+04</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "      <td>82436.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1999.957385</td>\n",
       "      <td>30.094631</td>\n",
       "      <td>95.828728</td>\n",
       "      <td>116.066207</td>\n",
       "      <td>40945.816997</td>\n",
       "      <td>37389.724877</td>\n",
       "      <td>5.281982e+06</td>\n",
       "      <td>0.183606</td>\n",
       "      <td>0.816394</td>\n",
       "      <td>0.146179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809306</td>\n",
       "      <td>69.666958</td>\n",
       "      <td>69.182528</td>\n",
       "      <td>5.445109</td>\n",
       "      <td>-5.568068</td>\n",
       "      <td>18.161273</td>\n",
       "      <td>-16.173655</td>\n",
       "      <td>-12.100269</td>\n",
       "      <td>-1.821947</td>\n",
       "      <td>-0.099322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.173337</td>\n",
       "      <td>13.765798</td>\n",
       "      <td>76.271186</td>\n",
       "      <td>41.992524</td>\n",
       "      <td>52971.576973</td>\n",
       "      <td>51188.743041</td>\n",
       "      <td>8.282445e+06</td>\n",
       "      <td>0.214977</td>\n",
       "      <td>0.214977</td>\n",
       "      <td>0.049615</td>\n",
       "      <td>...</td>\n",
       "      <td>1.301745</td>\n",
       "      <td>9.298144</td>\n",
       "      <td>11.897232</td>\n",
       "      <td>5.367811</td>\n",
       "      <td>1.569149</td>\n",
       "      <td>139.300324</td>\n",
       "      <td>165.382218</td>\n",
       "      <td>127.328276</td>\n",
       "      <td>21.688039</td>\n",
       "      <td>0.916226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1981.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.500000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.677460</td>\n",
       "      <td>14.085357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-13.082834</td>\n",
       "      <td>-820.350000</td>\n",
       "      <td>-1331.975333</td>\n",
       "      <td>-711.363333</td>\n",
       "      <td>-92.551390</td>\n",
       "      <td>-2.855399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1989.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>2.620000e+05</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.741554</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.932381</td>\n",
       "      <td>63.020982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-6.540371</td>\n",
       "      <td>-67.342500</td>\n",
       "      <td>-134.223819</td>\n",
       "      <td>-93.482417</td>\n",
       "      <td>-16.362113</td>\n",
       "      <td>-0.761073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>114.200000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>13400.000000</td>\n",
       "      <td>1.444300e+06</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376733</td>\n",
       "      <td>72.180899</td>\n",
       "      <td>71.905000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-5.513599</td>\n",
       "      <td>15.645000</td>\n",
       "      <td>-14.790250</td>\n",
       "      <td>-18.564667</td>\n",
       "      <td>-3.362340</td>\n",
       "      <td>-0.096809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>145.600000</td>\n",
       "      <td>60900.000000</td>\n",
       "      <td>54800.000000</td>\n",
       "      <td>6.758400e+06</td>\n",
       "      <td>0.258446</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.178244</td>\n",
       "      <td>75.935668</td>\n",
       "      <td>78.303125</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.494381</td>\n",
       "      <td>100.122500</td>\n",
       "      <td>99.468556</td>\n",
       "      <td>57.966667</td>\n",
       "      <td>10.533614</td>\n",
       "      <td>0.547280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>810.000000</td>\n",
       "      <td>277.100000</td>\n",
       "      <td>397000.000000</td>\n",
       "      <td>394000.000000</td>\n",
       "      <td>7.722400e+07</td>\n",
       "      <td>0.999468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540500</td>\n",
       "      <td>...</td>\n",
       "      <td>22.216051</td>\n",
       "      <td>86.296296</td>\n",
       "      <td>89.304286</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.158148</td>\n",
       "      <td>1005.720000</td>\n",
       "      <td>1786.769389</td>\n",
       "      <td>1199.514667</td>\n",
       "      <td>223.797594</td>\n",
       "      <td>3.135909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Year    State ANSI   County ANSI  Yield_BU_ACRE  \\\n",
       "count  82436.000000  82436.000000  82436.000000   82436.000000   \n",
       "mean    1999.957385     30.094631     95.828728     116.066207   \n",
       "std       12.173337     13.765798     76.271186      41.992524   \n",
       "min     1981.000000      1.000000      1.000000       0.000000   \n",
       "25%     1989.000000     19.000000     39.000000      86.000000   \n",
       "50%     1999.000000     29.000000     83.000000     114.200000   \n",
       "75%     2010.000000     42.000000    135.000000     145.600000   \n",
       "max     2023.000000     56.000000    810.000000     277.100000   \n",
       "\n",
       "       Area_Planted_ACRES  Area_Harvested_ACRES  Production_BU  \\\n",
       "count        82436.000000          82436.000000   8.243600e+04   \n",
       "mean         40945.816997          37389.724877   5.281982e+06   \n",
       "std          52971.576973          51188.743041   8.282445e+06   \n",
       "min             10.000000             10.000000   4.500000e+02   \n",
       "25%           4200.000000           2800.000000   2.620000e+05   \n",
       "50%          17000.000000          13400.000000   1.444300e+06   \n",
       "75%          60900.000000          54800.000000   6.758400e+06   \n",
       "max         397000.000000         394000.000000   7.722400e+07   \n",
       "\n",
       "       Abandonment_Rate  Harvest_Efficiency      Soil_AWC  ...  \\\n",
       "count      82436.000000        82436.000000  82436.000000  ...   \n",
       "mean           0.183606            0.816394      0.146179  ...   \n",
       "std            0.214977            0.214977      0.049615  ...   \n",
       "min            0.000000            0.000532      0.000000  ...   \n",
       "25%            0.031250            0.741554      0.115700  ...   \n",
       "50%            0.090909            0.909091      0.150000  ...   \n",
       "75%            0.258446            0.968750      0.178600  ...   \n",
       "max            0.999468            1.000000      0.540500  ...   \n",
       "\n",
       "       heat_moisture_stress       rh_mean  rh_reproductive  \\\n",
       "count          82436.000000  82436.000000     82436.000000   \n",
       "mean               0.809306     69.666958        69.182528   \n",
       "std                1.301745      9.298144        11.897232   \n",
       "min                0.000000     21.677460        14.085357   \n",
       "25%                0.000000     65.932381        63.020982   \n",
       "50%                0.376733     72.180899        71.905000   \n",
       "75%                1.178244     75.935668        78.303125   \n",
       "max               22.216051     86.296296        89.304286   \n",
       "\n",
       "       weeks_high_humidity  temp_early_vs_late  precip_early_vs_late  \\\n",
       "count         82436.000000        82436.000000          82436.000000   \n",
       "mean              5.445109           -5.568068             18.161273   \n",
       "std               5.367811            1.569149            139.300324   \n",
       "min               0.000000          -13.082834           -820.350000   \n",
       "25%               1.000000           -6.540371            -67.342500   \n",
       "50%               4.000000           -5.513599             15.645000   \n",
       "75%               8.000000           -4.494381            100.122500   \n",
       "max              52.000000            1.158148           1005.720000   \n",
       "\n",
       "        gdd_anomaly  precip_anomaly_mm  precip_anomaly_pct  temp_anomaly  \n",
       "count  82436.000000       82436.000000        82436.000000  82436.000000  \n",
       "mean     -16.173655         -12.100269           -1.821947     -0.099322  \n",
       "std      165.382218         127.328276           21.688039      0.916226  \n",
       "min    -1331.975333        -711.363333          -92.551390     -2.855399  \n",
       "25%     -134.223819         -93.482417          -16.362113     -0.761073  \n",
       "50%      -14.790250         -18.564667           -3.362340     -0.096809  \n",
       "75%       99.468556          57.966667           10.533614      0.547280  \n",
       "max     1786.769389        1199.514667          223.797594      3.135909  \n",
       "\n",
       "[8 rows x 47 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e382d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features for historical yield\n",
    "df = df.sort_values(['State', 'County', 'Year'])\n",
    "df['Yield_Lag1'] = df.groupby(['State', 'County'])['Yield_BU_ACRE'].shift(1)\n",
    "df['Yield_Lag2'] = df.groupby(['State', 'County'])['Yield_BU_ACRE'].shift(2)\n",
    "df['Yield_3yr_Avg'] = df.groupby(['State', 'County'])['Yield_BU_ACRE'].transform(\n",
    "    lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2ffd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Created lag features\n",
      "  âœ“ Records after feature engineering: 77,237\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with NaN in lag features\n",
    "df = df.dropna(subset=['Yield_Lag1', 'Yield_Lag2', 'Yield_3yr_Avg'])\n",
    "print(f\"  âœ“ Created lag features\")\n",
    "print(f\"  âœ“ Records after feature engineering: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f2e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Selected 44 features\n",
      "\n",
      "  Feature Groups:\n",
      "    Historical: Yield_Lag1, Yield_Lag2, Yield_3yr_Avg\n",
      "    Soil: Soil_AWC, Soil_Clay_Pct, Soil_pH, Soil_Organic_Matter_Pct\n",
      "    Weather: gdd_total, precip_total, temp_mean_season, etc. (34 features)\n",
      "    Other: Abandonment_Rate, Harvest_Efficiency, State_Encoded\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns\n",
    "exclude_cols = [\n",
    "    'Yield_BU_ACRE',  # Target\n",
    "    'State', 'County',  # Identifiers (will encode)\n",
    "    'Year',  # Already captured in features\n",
    "    'State ANSI', 'County ANSI', 'Ag District',  # Redundant IDs\n",
    "    'Area_Planted_ACRES', 'Area_Harvested_ACRES', 'Production_BU',  # Don't use for prediction\n",
    "]\n",
    "\n",
    "# Encode State as categorical\n",
    "state_encoder = {state: idx for idx, state in enumerate(df['State'].unique())}\n",
    "df['State_Encoded'] = df['State'].map(state_encoder)\n",
    "\n",
    "# Select features\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "print(f\"  âœ“ Selected {len(feature_cols)} features\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df[feature_cols]\n",
    "y = df['Yield_BU_ACRE']\n",
    "\n",
    "print(f\"\\n  Feature Groups:\")\n",
    "print(f\"    Historical: Yield_Lag1, Yield_Lag2, Yield_3yr_Avg\")\n",
    "print(f\"    Soil: Soil_AWC, Soil_Clay_Pct, Soil_pH, Soil_Organic_Matter_Pct\")\n",
    "print(f\"    Weather: gdd_total, precip_total, temp_mean_season, etc. (34 features)\")\n",
    "print(f\"    Other: Abandonment_Rate, Harvest_Efficiency, State_Encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfcb7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/8] Creating train/validation/test splits...\n",
      "  âœ“ Training set:   54,065 samples (70.0%)\n",
      "  âœ“ Validation set: 11,586 samples (15.0%)\n",
      "  âœ“ Test set:       11,586 samples (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. TRAIN/VAL/TEST SPLIT\n",
    "# ============================================================================\n",
    "print(\"\\n[3/8] Creating train/validation/test splits...\")\n",
    "\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Second split: separate validation set from training\n",
    "val_size_adjusted = VAL_SIZE / (1 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_size_adjusted, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"  âœ“ Training set:   {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  âœ“ Validation set: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  âœ“ Test set:       {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18434b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/8] Scaling features...\n",
      "  âœ“ Features scaled (StandardScaler)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. FEATURE SCALING\n",
    "# ============================================================================\n",
    "print(\"\\n[4/8] Scaling features...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(f\"  âœ“ Features scaled (StandardScaler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22a7ca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/8] Training models...\n",
      "======================================================================\n",
      "\n",
      "[Model 1/5] Baseline: Historical Average\n",
      "----------------------------------------------------------------------\n",
      "  Training   - MAE: 19.30, RMSE: 25.31, RÂ²: 0.640\n",
      "  Validation - MAE: 19.68, RMSE: 25.85, RÂ²: 0.626\n",
      "  Test       - MAE: 19.44, RMSE: 25.64, RÂ²: 0.628\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. MODEL TRAINING & EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n[5/8] Training models...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# MODEL 1: BASELINE - Historical Average\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Model 1/5] Baseline: Historical Average\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Use 3-year average as baseline prediction\n",
    "baseline_pred_train = X_train['Yield_3yr_Avg']\n",
    "baseline_pred_val = X_val['Yield_3yr_Avg']\n",
    "baseline_pred_test = X_test['Yield_3yr_Avg']\n",
    "\n",
    "# Metrics\n",
    "baseline_metrics = {\n",
    "    'train_mae': mean_absolute_error(y_train, baseline_pred_train),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, baseline_pred_train)),\n",
    "    'train_r2': r2_score(y_train, baseline_pred_train),\n",
    "    'val_mae': mean_absolute_error(y_val, baseline_pred_val),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, baseline_pred_val)),\n",
    "    'val_r2': r2_score(y_val, baseline_pred_val),\n",
    "    'test_mae': mean_absolute_error(y_test, baseline_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, baseline_pred_test)),\n",
    "    'test_r2': r2_score(y_test, baseline_pred_test)\n",
    "}\n",
    "\n",
    "print(f\"  Training   - MAE: {baseline_metrics['train_mae']:.2f}, RMSE: {baseline_metrics['train_rmse']:.2f}, RÂ²: {baseline_metrics['train_r2']:.3f}\")\n",
    "print(f\"  Validation - MAE: {baseline_metrics['val_mae']:.2f}, RMSE: {baseline_metrics['val_rmse']:.2f}, RÂ²: {baseline_metrics['val_r2']:.3f}\")\n",
    "print(f\"  Test       - MAE: {baseline_metrics['test_mae']:.2f}, RMSE: {baseline_metrics['test_rmse']:.2f}, RÂ²: {baseline_metrics['test_r2']:.3f}\")\n",
    "\n",
    "metrics['Baseline'] = baseline_metrics\n",
    "predictions['Baseline'] = {'train': baseline_pred_train, 'val': baseline_pred_val, 'test': baseline_pred_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a805f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model 2/5] Ridge Regression with Hyperparameter Tuning\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Best parameters: {'alpha': 10.0}\n",
      "  âœ“ Best CV RÂ²: 0.722\n",
      "  Training   - MAE: 16.74, RMSE: 22.22, RÂ²: 0.723\n",
      "  Validation - MAE: 17.15, RMSE: 22.80, RÂ²: 0.709\n",
      "  Test       - MAE: 16.94, RMSE: 22.51, RÂ²: 0.713\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# MODEL 2: RIDGE REGRESSION\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Model 2/5] Ridge Regression with Hyperparameter Tuning\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "ridge_model = Ridge(random_state=RANDOM_STATE)\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    ridge_model, param_grid_ridge, cv=CV_FOLDS, \n",
    "    scoring='r2', n_jobs=-1, verbose=0\n",
    ")\n",
    "grid_search_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"  âœ“ Best parameters: {grid_search_ridge.best_params_}\")\n",
    "print(f\"  âœ“ Best CV RÂ²: {grid_search_ridge.best_score_:.3f}\")\n",
    "\n",
    "# Train final model\n",
    "ridge_best = grid_search_ridge.best_estimator_\n",
    "ridge_pred_train = ridge_best.predict(X_train_scaled)\n",
    "ridge_pred_val = ridge_best.predict(X_val_scaled)\n",
    "ridge_pred_test = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "ridge_metrics = {\n",
    "    'train_mae': mean_absolute_error(y_train, ridge_pred_train),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, ridge_pred_train)),\n",
    "    'train_r2': r2_score(y_train, ridge_pred_train),\n",
    "    'val_mae': mean_absolute_error(y_val, ridge_pred_val),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, ridge_pred_val)),\n",
    "    'val_r2': r2_score(y_val, ridge_pred_val),\n",
    "    'test_mae': mean_absolute_error(y_test, ridge_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, ridge_pred_test)),\n",
    "    'test_r2': r2_score(y_test, ridge_pred_test)\n",
    "}\n",
    "\n",
    "print(f\"  Training   - MAE: {ridge_metrics['train_mae']:.2f}, RMSE: {ridge_metrics['train_rmse']:.2f}, RÂ²: {ridge_metrics['train_r2']:.3f}\")\n",
    "print(f\"  Validation - MAE: {ridge_metrics['val_mae']:.2f}, RMSE: {ridge_metrics['val_rmse']:.2f}, RÂ²: {ridge_metrics['val_r2']:.3f}\")\n",
    "print(f\"  Test       - MAE: {ridge_metrics['test_mae']:.2f}, RMSE: {ridge_metrics['test_rmse']:.2f}, RÂ²: {ridge_metrics['test_r2']:.3f}\")\n",
    "\n",
    "models['Ridge'] = ridge_best\n",
    "metrics['Ridge'] = ridge_metrics\n",
    "predictions['Ridge'] = {'train': ridge_pred_train, 'val': ridge_pred_val, 'test': ridge_pred_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1416874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model 3/5] Random Forest with Hyperparameter Tuning (Randomized Search)\n",
      "----------------------------------------------------------------------\n",
      "  Training Random Forest (much faster now)...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x105701da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Best parameters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n",
      "  âœ“ Best CV RÂ²: 0.840\n",
      "  Training   - MAE: 5.49, RMSE: 7.86, RÂ²: 0.965\n",
      "  Validation - MAE: 12.42, RMSE: 17.07, RÂ²: 0.837\n",
      "  Test       - MAE: 12.14, RMSE: 16.62, RÂ²: 0.844\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# MODEL 3: RANDOM FOREST (Randomized Search)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Model 3/5] Random Forest with Hyperparameter Tuning (Randomized Search)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200, 300, 400],  # more options, random picks are cheap\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # optional but often improves RF\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# ðŸ”¥ Randomized search instead of grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=12,              # number of random combos to try (tune if needed)\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"  Training Random Forest (much faster now)...\")\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"  âœ“ Best parameters: {random_search_rf.best_params_}\")\n",
    "print(f\"  âœ“ Best CV RÂ²: {random_search_rf.best_score_:.3f}\")\n",
    "\n",
    "rf_best = random_search_rf.best_estimator_\n",
    "\n",
    "rf_pred_train = rf_best.predict(X_train)\n",
    "rf_pred_val = rf_best.predict(X_val)\n",
    "rf_pred_test = rf_best.predict(X_test)\n",
    "\n",
    "rf_metrics = {\n",
    "    'train_mae': mean_absolute_error(y_train, rf_pred_train),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, rf_pred_train)),\n",
    "    'train_r2': r2_score(y_train, rf_pred_train),\n",
    "    'val_mae': mean_absolute_error(y_val, rf_pred_val),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, rf_pred_val)),\n",
    "    'val_r2': r2_score(y_val, rf_pred_val),\n",
    "    'test_mae': mean_absolute_error(y_test, rf_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, rf_pred_test)),\n",
    "    'test_r2': r2_score(y_test, rf_pred_test)\n",
    "}\n",
    "\n",
    "print(f\"  Training   - MAE: {rf_metrics['train_mae']:.2f}, RMSE: {rf_metrics['train_rmse']:.2f}, RÂ²: {rf_metrics['train_r2']:.3f}\")\n",
    "print(f\"  Validation - MAE: {rf_metrics['val_mae']:.2f}, RMSE: {rf_metrics['val_rmse']:.2f}, RÂ²: {rf_metrics['val_r2']:.3f}\")\n",
    "print(f\"  Test       - MAE: {rf_metrics['test_mae']:.2f}, RMSE: {rf_metrics['test_rmse']:.2f}, RÂ²: {rf_metrics['test_r2']:.3f}\")\n",
    "\n",
    "models['RandomForest'] = rf_best\n",
    "metrics['RandomForest'] = rf_metrics\n",
    "predictions['RandomForest'] = {'train': rf_pred_train, 'val': rf_pred_val, 'test': rf_pred_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c90eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model 4/5] XGBoost with Randomized Hyperparameter Search\n",
      "----------------------------------------------------------------------\n",
      "  Training XGBoost (this should be faster than grid search, hopefully saving your sanity)...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "  âœ“ Best parameters: {'subsample': 0.8, 'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.6}\n",
      "  âœ“ Best CV RÂ²: 0.858\n",
      "  Training   - MAE: 2.04, RMSE: 2.76, RÂ²: 0.996\n",
      "  Validation - MAE: 11.44, RMSE: 15.81, RÂ²: 0.860\n",
      "  Test       - MAE: 11.22, RMSE: 15.59, RÂ²: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x103469da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104afdda0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x102981da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# MODEL 4: XGBOOST (Randomized Hyperparameter Search)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Model 4/5] XGBoost with Randomized Hyperparameter Search\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': [100, 200, 300, 400],   # wider search = good vibes\n",
    "    'max_depth': [3, 5, 7, 10, 15],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],    # throwing in an extra buff for the model\n",
    "    'gamma': [0, 1, 5]                      # optional spice\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'  # faster training, GPU-friendly if available\n",
    ")\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=20,         # number of random combos to try\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"  Training XGBoost (this should be faster than grid search, hopefully saving your sanity)...\")\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"  âœ“ Best parameters: {random_search_xgb.best_params_}\")\n",
    "print(f\"  âœ“ Best CV RÂ²: {random_search_xgb.best_score_:.3f}\")\n",
    "\n",
    "xgb_best = random_search_xgb.best_estimator_\n",
    "xgb_pred_train = xgb_best.predict(X_train)\n",
    "xgb_pred_val = xgb_best.predict(X_val)\n",
    "xgb_pred_test = xgb_best.predict(X_test)\n",
    "\n",
    "xgb_metrics = {\n",
    "    'train_mae': mean_absolute_error(y_train, xgb_pred_train),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, xgb_pred_train)),\n",
    "    'train_r2': r2_score(y_train, xgb_pred_train),\n",
    "    'val_mae': mean_absolute_error(y_val, xgb_pred_val),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, xgb_pred_val)),\n",
    "    'val_r2': r2_score(y_val, xgb_pred_val),\n",
    "    'test_mae': mean_absolute_error(y_test, xgb_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, xgb_pred_test)),\n",
    "    'test_r2': r2_score(y_test, xgb_pred_test)\n",
    "}\n",
    "\n",
    "print(f\"  Training   - MAE: {xgb_metrics['train_mae']:.2f}, RMSE: {xgb_metrics['train_rmse']:.2f}, RÂ²: {xgb_metrics['train_r2']:.3f}\")\n",
    "print(f\"  Validation - MAE: {xgb_metrics['val_mae']:.2f}, RMSE: {xgb_metrics['val_rmse']:.2f}, RÂ²: {xgb_metrics['val_r2']:.3f}\")\n",
    "print(f\"  Test       - MAE: {xgb_metrics['test_mae']:.2f}, RMSE: {xgb_metrics['test_rmse']:.2f}, RÂ²: {xgb_metrics['test_r2']:.3f}\")\n",
    "\n",
    "models['XGBoost'] = xgb_best\n",
    "metrics['XGBoost'] = xgb_metrics\n",
    "predictions['XGBoost'] = {'train': xgb_pred_train, 'val': xgb_pred_val, 'test': xgb_pred_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b1482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model 5/5] Gradient Boosting with Randomized Hyperparameter Search\n",
      "----------------------------------------------------------------------\n",
      "  Training Gradient Boosting (should be faster and less painful than grid search)...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x106961da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x107da1da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104ad1da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104235da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x102a55da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1062f5da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x106db5da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1045f5da0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Best parameters: {'subsample': 1.0, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 10, 'learning_rate': 0.1}\n",
      "  âœ“ Best CV RÂ²: 0.853\n",
      "  Training   - MAE: 4.86, RMSE: 6.35, RÂ²: 0.977\n",
      "  Validation - MAE: 11.79, RMSE: 16.22, RÂ²: 0.853\n",
      "  Test       - MAE: 11.38, RMSE: 15.82, RÂ²: 0.859\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# MODEL 5: GRADIENT BOOSTING (Randomized Hyperparameter Search)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[Model 5/5] Gradient Boosting with Randomized Hyperparameter Search\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],       # new knob for more fun\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    gb_model,\n",
    "    param_distributions=param_dist_gb,\n",
    "    n_iter=20,            # number of random combos to explore\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"  Training Gradient Boosting (should be faster and less painful than grid search)...\")\n",
    "random_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"  âœ“ Best parameters: {random_search_gb.best_params_}\")\n",
    "print(f\"  âœ“ Best CV RÂ²: {random_search_gb.best_score_:.3f}\")\n",
    "\n",
    "gb_best = random_search_gb.best_estimator_\n",
    "gb_pred_train = gb_best.predict(X_train)\n",
    "gb_pred_val = gb_best.predict(X_val)\n",
    "gb_pred_test = gb_best.predict(X_test)\n",
    "\n",
    "gb_metrics = {\n",
    "    'train_mae': mean_absolute_error(y_train, gb_pred_train),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, gb_pred_train)),\n",
    "    'train_r2': r2_score(y_train, gb_pred_train),\n",
    "    'val_mae': mean_absolute_error(y_val, gb_pred_val),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val, gb_pred_val)),\n",
    "    'val_r2': r2_score(y_val, gb_pred_val),\n",
    "    'test_mae': mean_absolute_error(y_test, gb_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, gb_pred_test)),\n",
    "    'test_r2': r2_score(y_test, gb_pred_test)\n",
    "}\n",
    "\n",
    "print(f\"  Training   - MAE: {gb_metrics['train_mae']:.2f}, RMSE: {gb_metrics['train_rmse']:.2f}, RÂ²: {gb_metrics['train_r2']:.3f}\")\n",
    "print(f\"  Validation - MAE: {gb_metrics['val_mae']:.2f}, RMSE: {gb_metrics['val_rmse']:.2f}, RÂ²: {gb_metrics['val_r2']:.3f}\")\n",
    "print(f\"  Test       - MAE: {gb_metrics['test_mae']:.2f}, RMSE: {gb_metrics['test_rmse']:.2f}, RÂ²: {gb_metrics['test_r2']:.3f}\")\n",
    "\n",
    "models['GradientBoosting'] = gb_best\n",
    "metrics['GradientBoosting'] = gb_metrics\n",
    "predictions['GradientBoosting'] = {'train': gb_pred_train, 'val': gb_pred_val, 'test': gb_pred_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4bfc92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "[6/8] MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Test Set Performance (sorted by RÂ²):\n",
      "           Model  Test_R2  Test_MAE  Test_RMSE\n",
      "         XGBoost 0.862570 11.220703  15.588145\n",
      "GradientBoosting 0.858527 11.380200  15.815724\n",
      "    RandomForest 0.843787 12.142737  16.619238\n",
      "           Ridge 0.713413 16.935542  22.510298\n",
      "        Baseline 0.628151 19.436423  25.641092\n",
      "\n",
      "======================================================================\n",
      "BEST MODEL (based on Test RÂ²):\n",
      "  XGBoost\n",
      "  RÂ² = 0.863\n",
      "  MAE = 11.22 BU/ACRE\n",
      "======================================================================\n",
      "\n",
      "âœ“ Saved: model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[6/8] MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, model_metrics in metrics.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train_R2': model_metrics['train_r2'],\n",
    "        'Train_MAE': model_metrics['train_mae'],\n",
    "        'Train_RMSE': model_metrics['train_rmse'],\n",
    "        'Val_R2': model_metrics['val_r2'],\n",
    "        'Val_MAE': model_metrics['val_mae'],\n",
    "        'Val_RMSE': model_metrics['val_rmse'],\n",
    "        'Test_R2': model_metrics['test_r2'],\n",
    "        'Test_MAE': model_metrics['test_mae'],\n",
    "        'Test_RMSE': model_metrics['test_rmse']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(\"\\nTest Set Performance (sorted by RÂ²):\")\n",
    "print(comparison_df[['Model', 'Test_R2', 'Test_MAE', 'Test_RMSE']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL (based on Test RÂ²):\")\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model_r2 = comparison_df.iloc[0]['Test_R2']\n",
    "best_model_mae = comparison_df.iloc[0]['Test_MAE']\n",
    "print(f\"  {best_model_name}\")\n",
    "print(f\"  RÂ² = {best_model_r2:.3f}\")\n",
    "print(f\"  MAE = {best_model_mae:.2f} BU/ACRE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: model_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4cc9d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/8] Feature Importance Analysis...\n",
      "\n",
      "Top 15 Most Important Features (XGBoost):\n",
      "               Feature  Importance\n",
      "         Yield_3yr_Avg    0.395422\n",
      "            Yield_Lag1    0.107284\n",
      "  heat_moisture_stress    0.037020\n",
      "            Yield_Lag2    0.030552\n",
      "    weeks_extreme_heat    0.024783\n",
      "     weeks_heat_stress    0.022966\n",
      "     precip_anomaly_mm    0.018062\n",
      "temp_mean_reproductive    0.017513\n",
      "         State_Encoded    0.017056\n",
      "       temp_std_season    0.015649\n",
      "   weeks_high_humidity    0.015281\n",
      "           gdd_anomaly    0.015204\n",
      "   precip_reproductive    0.014911\n",
      "          temp_anomaly    0.014103\n",
      "       rh_reproductive    0.014037\n",
      "\n",
      "âœ“ Saved: feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[7/8] Feature Importance Analysis...\")\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "if 'XGBoost' in models:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': models['XGBoost'].feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features (XGBoost):\")\n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "    print(\"\\nâœ“ Saved: feature_importance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac1d4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/8] Saving models...\n",
      "  âœ“ Saved: saved_models/ridge_model.pkl\n",
      "  âœ“ Saved: saved_models/randomforest_model.pkl\n",
      "  âœ“ Saved: saved_models/xgboost_model.pkl\n",
      "  âœ“ Saved: saved_models/gradientboosting_model.pkl\n",
      "  âœ“ Saved: saved_models/scaler.pkl\n",
      "  âœ“ Saved: saved_models/feature_columns.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[8/8] Saving models...\")\n",
    "\n",
    "# Save all models\n",
    "for model_name, model in models.items():\n",
    "    filename = f'saved_models/{model_name.lower()}_model.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"  âœ“ Saved: {filename}\")\n",
    "\n",
    "# Save scaler\n",
    "with open('saved_models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"  âœ“ Saved: saved_models/scaler.pkl\")\n",
    "\n",
    "# Save feature columns\n",
    "with open('saved_models/feature_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(list(X_train.columns), f)\n",
    "print(f\"  âœ“ Saved: saved_models/feature_columns.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "085f96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "âœ“ Trained 5 models\n",
      "âœ“ Best model: XGBoost (RÂ² = 0.863)\n",
      "âœ“ Improvement over baseline: 0.234\n",
      "âœ“ All models saved to saved_models/\n",
      "âœ“ Comparison table: model_comparison.csv\n",
      "âœ“ Feature importance: feature_importance.csv\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ“ Trained 5 models\")\n",
    "print(f\"âœ“ Best model: {best_model_name} (RÂ² = {best_model_r2:.3f})\")\n",
    "print(f\"âœ“ Improvement over baseline: {(best_model_r2 - baseline_metrics['test_r2']):.3f}\")\n",
    "print(f\"âœ“ All models saved to saved_models/\")\n",
    "print(f\"âœ“ Comparison table: model_comparison.csv\")\n",
    "print(f\"âœ“ Feature importance: feature_importance.csv\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8713c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
